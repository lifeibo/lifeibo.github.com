
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>李子的博客</title>
  <meta name="author" content="lifeibo">

  
  <meta name="description" content="致力于高性能网络服务器的优化与研究。">
  
<meta name="keywords" content="李飞勃 nginx 网络 源码 学习 lifeibo push comet Linux Apache">

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://blog.lifeibo.com/index.html">
  <link href="/favicon.ico" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/highlight/googlecode.css" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="李子的博客" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">李子的博客</a></h1>
  
    <h2>想想写写</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:blog.lifeibo.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about/">about</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/12/19/slab-usage-alloc-larger-memory.html">Nginx中slab分配大内存的陷阱</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-12-19T17:57:00+08:00" pubdate data-updated="true">Dec 19<span>th</span>, 2012</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/nginx/'>nginx</a>
  
</span>


      
  

<span class="byline author vcard">Posted by <span class="fn">lifeibo</span></span>

        
      </p>
    
  </header>


  <div class="entry-content"><p>我们在开发nginx模块时，需要很小心，nginx里面有很多陷阱是我们需要注意的。之前有人提到过slab分配器在使用时，不适合大内存分配，否则会出现分配不出内存的现象。</p>

<p>nginx一般使用slab来管理共享内存，在程序启动时，很分配好需要共享的内存，然后使用slab来进行初始化，之后就交给slab来管理这段内存。slab的源码分析与合适，在我之前的博客里面有分析过。这次我们针对性的看看，为什么会出现分配不出内存的现象。</p>

<p>分配内存时，会调用<code>ngx_slab_alloc_locked</code>，在这个函数里面会先判断size是否大于<code>ngx_slab_max_size</code>，代码如下。</p>

<pre><code>void *
ngx_slab_alloc_locked(ngx_slab_pool_t *pool, size_t size)
{
    size_t            s;
    uintptr_t         p, n, m, mask, *bitmap;
    ngx_uint_t        i, slot, shift, map;
    ngx_slab_page_t  *page, *prev, *slots;

    /* 判断大小 */
    if (size &gt;= ngx_slab_max_size) {

        ngx_log_debug1(NGX_LOG_DEBUG_ALLOC, ngx_cycle-&gt;log, 0,
                       "slab alloc: %uz", size);

        /* 直接分配页 */
        page = ngx_slab_alloc_pages(pool, (size + ngx_pagesize - 1)
                &gt;&gt; ngx_pagesize_shift);
        if (page) {
            p = (page - pool-&gt;pages) &lt;&lt; ngx_pagesize_shift;
            p += (uintptr_t) pool-&gt;start;

        } else {
            p = 0;
        }

        goto done;
    }

    ...

}
</code></pre>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2012/12/19/slab-usage-alloc-larger-memory.html">Read more &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/12/17/nginx-varibles.html">Nginx源码分析之变量</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-12-17T18:51:00+08:00" pubdate data-updated="true">Dec 17<span>th</span>, 2011</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/nginx/'>nginx</a>
  
</span>


      
  

<span class="byline author vcard">Posted by <span class="fn">lifeibo</span></span>

        
      </p>
    
  </header>


  <div class="entry-content"><p>nginx中的变量在nginx中的使用非常的多，正因为变量的存在，使得nginx在配置上变得非常灵活。</p>

<p>我们知道，在nginx的配置文件中，配合变量，我们可以动态的得到我们想要的值。最常见的使用是，我们在写<code>access_log</code>的格式时，需要用到多很多变量。
而这些变量是如何工作的呢？我们可以输出哪些变量？我们又怎么才能输出自己想要的内容呢？当然，我们可能还想知道，如何在我们的模块里面去使用变量，如何添加变量，获取变量的值，以及设置变量的内容？如何使用，以及需要注意些什么？</p>

<p>问题一大堆，那接下来，就让我们一起去一探nginx源码的秘密。</p>

<p>我要讲的内容</p>

<ol>
<li>变量的分类</li>
<li>相关结构</li>
<li>模块中操作变量的函数</li>
<li>变量的实现源码及流程</li>
</ol>


<h2>1. 变量的分类</h2>

<p>站在使用者的角度来看，我们在配置文件中可以看到：</p>

<ol>
<li>set添加的变量(变量名由用户设定)</li>
<li>nginx功能模块中添加的变量，如geo模块(变量名由用户设定)</li>
<li>nginx内建的变量(变量名已由nginx设定好，可以看<code>ngx_http_core_variables</code>结构)</li>
<li>有一定规则的变量，如”<code>http_host</code>”等(有相同前缀，表示某一类变量)，我们就称为规则变量吧</li>
</ol>


<p>从这里，也解决我们的问题，在配置<code>access_log</code>时，我们可以配置哪些变量，是否是用户添加的变量，是否是内建变量在<code>ngx_http_core_variables</code>中有，其次，是否是规则变量，另外，如果想输出自己的内容，那只能写模块自己添加一个变量了，或者hack nginx在<code>ngx_http_core_variables</code>中添加一个变量。</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2011/12/17/nginx-varibles.html">Read more &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/09/09/port-reuse.html">端口重用引发的悲剧</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-09-09T17:57:00+08:00" pubdate data-updated="true">Sep 9<span>th</span>, 2011</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/linux/'>linux</a>
  
</span>


      
  

<span class="byline author vcard">Posted by <span class="fn">lifeibo</span></span>

        
      </p>
    
  </header>


  <div class="entry-content"><p>最近做个性能测试，需要在一台机器上启动很多客户端，连接到同一台服务器，我在一台机器上启动了六万个连接，于是，端口被占用完了。按照我的理解，因为我作用端口是作为客户端，应该不会影响到其它进程，于是我放心大胆地去做测试，结果就引发了悲剧。有服务器程序要用到5191端口，却显示端口被占用了，lsof看了下，居然只有我的进程占用了，完全颠覆我的惯性思想。服务端与客户端都有打开<code>SO_REUSEADDR</code>。</p>

<p>我们先来看看<code>SO_REUSEADDR</code>的说明：</p>

<pre><code>SO_REUSEADDR可以用在以下四种情况下。
(摘自《Unix网络编程》卷一，即UNPv1)
1、当有一个有相同本地地址和端口的socket1处于TIME_WAIT状态时，而你启
动的程序的socket2要占用该地址和端口，你的程序就要用到该选项。
2、SO_REUSEADDR允许同一port上启动同一服务器的多个实例(多个进程)。但
每个实例绑定的IP地址是不能相同的。在有多块网卡或用IP Alias技术的机器可
以测试这种情况。
3、SO_REUSEADDR允许单个进程绑定相同的端口到多个socket上，但每个soc
ket绑定的ip地址不同。这和2很相似，区别请看UNPv1。
4、SO_REUSEADDR允许完全相同的地址和端口的重复绑定。但这只用于UDP的
多播，不用于TCP。
</code></pre>

<p>分析了一下，第二种情况比较相似，但第二种情况是针对同一服务器的多个实例(多个进程)，很显然跟我的情况不一样。我是在客户端与服务端间争用端口。 所以，我的情况根本就不适合以上任何一种情况，所以设置<code>SO_REUSEADDR</code>为1是无用的。</p>

<p>再接下来分析：
一个TCP连接需要由四元组来形成，即(<code>src_ip</code>,<code>src_port</code>,<code>dst_ip,dst_port</code>)。
如果有客户端建立了连接(<code>src_ip1</code>,<code>src_port1</code>,<code>dst_ip1</code>,<code>dst_port1</code>)，那么，如果我们还有listen在(<code>src_ip1</code>,<code>src_port1</code>)，那么当(<code>dst_ip1</code>,<code>dst_port1</code>)发送消息过来，系统应该把消息给谁？所以就说明了客户端占用了某一端口时，该端口就不能被其它进程listen了。</p>

<p>那么，对于有些童鞋，可能还有这样的疑问，是否一台机器就只能建立65535个连接了（端口16位限制）？非也，一个连接由四元组(<code>src_ip</code>,<code>src_port</code>,<code>dst_ip</code>,<code>dst_port</code>)形式，那么当(<code>src_ip</code>,<code>src_port</code>)一定时，变化的(<code>dst_ip</code>,<code>dst_port</code>)就可以建立更多连接了。</p>

<p>可能有些童鞋还有疑问，作为一个服务器监控一个端口，比如80端口，它为什么可以建立上百万个连接？首先要明白一点，当accept出来后的新socket，它所占用的本地端口依然是80端口，很多新手都以为是一个新的随机端口。由四元组就很容易分析到了，同一个(<code>src_ip</code>,<code>src_port</code>)，它所对应的(<code>dst_ip</code>,<code>dst_port</code>)可以无穷变化，这样就可以建立很多个客户端的请求了。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/08/29/net-work.html">统计网卡流量</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-08-29T17:57:00+08:00" pubdate data-updated="true">Aug 29<span>th</span>, 2011</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/linux/'>linux</a>
  
</span>


      
  

<span class="byline author vcard">Posted by <span class="fn">lifeibo</span></span>

        
      </p>
    
  </header>


  <div class="entry-content"><p>显示网卡流量的方法蛮多，一般我们可以通过dstat来查看，但dstat不一定所有的机器都有安装。而我们知道，通过ifconfig可以看到某一网卡发送与接收的字节数，所以我们可以写一个脚本来统计一下。</p>

<p>先看ifconfig:</p>

<pre><code>$ ifconfig eth0  
eth0      Link encap:Ethernet  HWaddr A4:BA:DB:43:BA:B1  
          inet addr:10.232.4.34  Bcast:10.232.4.255  Mask:255.255.255.0  
          inet6 addr: fe80::a6ba:dbff:fe43:bab1/64 Scope:Link  
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1  
          RX packets:301707081 errors:0 dropped:1346358 overruns:0 frame:0  
          TX packets:296718885 errors:0 dropped:0 overruns:0 carrier:0  
          collisions:0 txqueuelen:1000  
          RX bytes:28485042645 (26.5 GiB)  TX bytes:35887266717 (33.4 GiB)  
          Interrupt:98 Memory:d6000000-d6012800  
</code></pre>

<p>我们可以看到rx与tx两个数据，于是我们的脚本出来了：</p>

<pre><code>#!/bin/bash
alias ifconfig="/sbin/ifconfig"
eth=eth0
while true; do
RXpre=$(ifconfig ${eth} | grep bytes | awk '{print $2}'| awk -F":" '{print $2}')
TXpre=$(ifconfig ${eth} | grep bytes | awk '{print $6}' | awk -F":" '{print $2}')
sleep 1
RXnext=$(ifconfig ${eth} | grep bytes | awk '{print $2}'| awk -F":" '{print $2}')
TXnext=$(ifconfig ${eth} | grep bytes | awk '{print $6}' | awk -F":" '{print $2}')
echo RX ----- TX
echo "$((((${RXnext}-${RXpre})/1024)/1024))MB/s $((((${TXnext}-${TXpre})/1024/1024)))MB/s"
done
</code></pre>

<p>脚本暂时比较简单，可以添加一些参数判断，比如多长时间显示一次等等，先看看执行结果:</p>

<pre><code>$ ./a  
RX ----- TX  
5MB/s 7MB/s  
RX ----- TX  
5MB/s 7MB/s  
RX ----- TX  
4MB/s 6MB/s  
RX ----- TX  
4MB/s 6MB/s  
RX ----- TX  
</code></pre>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/07/07/200-long-connection.html">Http长连接200万尝试及调优</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-07-07T17:57:00+08:00" pubdate data-updated="true">Jul 7<span>th</span>, 2011</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/http/'>http</a>
  
</span>


      
  

<span class="byline author vcard">Posted by <span class="fn">lifeibo</span></span>

        
      </p>
    
  </header>


  <div class="entry-content"><p>对于一个server，我们一般考虑他所能支撑的qps，但有那么一种应用， 我们需要关注的是它能支撑的连接数个数，而并非qps，当然qps也是我们需要考虑的性能点之一。这种应用常见于消息推送系统，也称为comet应用，比如聊天室或即时消息推送系统等。comet应用具体可见我之前的介绍，在此不多讲。对于这类系统，因为很多消息需要到产生时才推送给客户端，所以当没有消息产生时，就需要hold住客户端的连接，这样，当有大量的客户端时，就需要hold住大量的连接，这种连接我们称为长连接。</p>

<p>首先，我们分析一下，对于这类服务，需消耗的系统资源有：cpu、网络、内存。所以，想让系统性能达到最佳，我们先找到系统的瓶颈所在。这样的长连接，往往我们是没有数据发送的，所以也可以看作为非活动连接。对于系统来说，这种非活动连接，并不占用cpu与网络资源，而仅仅占用系统的内存而已。所以，我们假想，只要系统内存足够，系统就能够支持我们想达到的连接数，那么事实是否真的如此？如果真能这样，内核来维护这相当大的数据结构，也是一种考验。</p>

<p>要完成测试，我们需要有一个服务端，还有大量的客户端。所以需要服务端程序与客户端程序。为达到目标，我的想法是这样的：客户端产生一个连接，向服务端发起一个请求，服务端hold住该连接，而不返回数据。</p>

<h2>1. 服务端的准备</h2>

<p>对于服务端，由于之前的假想，我们需要一台大内存的服务器，用于部署nginx的comet应用。下面是我用的服务端的情况：</p>

<pre><code>Summary:        Dell R710, 2 x Xeon E5520 2.27GHz, 23.5GB / 24GB 1333MHz  
System:         Dell PowerEdge R710 (Dell 0VWN1R)  
Processors:     2 x Xeon E5520 2.27GHz 5860MHz FSB (16 cores)  
Memory:         23.5GB / 24GB 1333MHz == 6 x 4GB, 12 x empty  
Disk-Control:   megaraid_sas0: Dell/LSILogic PERC 6/i, Package 6.2.0-0013, FW 1.22.02-0612,  
Network:        eth0 (bnx2):Broadcom NetXtreme II BCM5709 Gigabit Ethernet,1000Mb/s  
OS:             RHEL Server 5.4 (Tikanga), Linux 2.6.18-164.el5 x86_64, 64-bit  
</code></pre>

<p>服务端程序很简单，基于nginx写的一个comet模块，该模块接受用户的请求，然后保持用户的连接，而不返回。Nginx的status模块，可直接用于监控最大连接数。</p>

<p>服务端还需要调整一下系统的参数，在/etc/sysctl.conf中：</p>

<pre><code>net.core.somaxconn = 2048  
net.core.rmem_default = 262144  
net.core.wmem_default = 262144  
net.core.rmem_max = 16777216  
net.core.wmem_max = 16777216  
net.ipv4.tcp_rmem = 4096 4096 16777216  
net.ipv4.tcp_wmem = 4096 4096 16777216  
net.ipv4.tcp_mem = 786432 2097152 3145728  
net.ipv4.tcp_max_syn_backlog = 16384  
net.core.netdev_max_backlog = 20000  
net.ipv4.tcp_fin_timeout = 15  
net.ipv4.tcp_max_syn_backlog = 16384  
net.ipv4.tcp_tw_reuse = 1  
net.ipv4.tcp_tw_recycle = 1  
net.ipv4.tcp_max_orphans = 131072  

/sbin/sysctl -p 生效
</code></pre>

<p>这里，我们主要看这几项：<br/>
<code>net.ipv4.tcp_rmem</code> 用来配置读缓冲的大小，三个值，第一个是这个读缓冲的最小值，第三个是最大值，中间的是默认值。我们可以在程序中修改读缓冲的大小，但是不能超过最小与最大。为了使每个socket所使用的内存数最小，我这里设置默认值为4096。<br/>
<code>net.ipv4.tcp_wmem</code> 用来配置写缓冲的大小。<br/>
读缓冲与写缓冲在大小，直接影响到socket在内核中内存的占用。<br/>
而<code>net.ipv4.tcp_mem</code>则是配置tcp的内存大小，其单位是页，而不是字节。当超过第二个值时，TCP进入pressure模式，此时TCP尝试稳定其内存的使用，当小于第一个值时，就退出pressure模式。当内存占用超过第三个值时，TCP就拒绝分配socket了，查看dmesg，会打出很多的日志“TCP: too many of orphaned sockets”。<br/>
另外<code>net.ipv4.tcp_max_orphans</code>这个值也要设置一下，这个值表示系统所能处理不属于任何进程的socket数量，当我们需要快速建立大量连接时，就需要关注下这个值了。当不属于任何进程的socket的数量大于这个值时，dmesg就会看到”too many of orphaned sockets”。</p>

<p>另外，服务端需要打开大量的文件描述符，比如200万个，但我们设置最大文件描述符限制时，会遇到一些问题，我们在后面详细讲解。</p>

<h2>2. 客户端的准备</h2>

<p>由于我们需要构建大量的客户端，而我们知道，在一台系统上，连接到一个服务时的本地端口是有限的。由于端口是16位整数，也就只能是0到65535，而0到1023是预留端口，所以能分配的只是1024到65534，也就是64511个。也就是说，一台机器只能创建六万多个长连接。要达到我们的两百万连接，需要大概34台客户端。<br/>
当然，我们可以采用虚拟ip的方式来实现这么多客户端，如果是虚拟ip，则每个ip可以绑定六万多个端口，34个虚拟ip就可以搞定。而我这里呢，正好申请到了公司的资源，所以就采用实体机来做了。<br/>
由于系统默认参数，自动分配的端口数有限，是从32768到61000，所以我们需要更改客户端/etc/sysctl.conf的参数：</p>

<pre><code>net.ipv4.ip_local_port_range = 1024 65535  

/sbin/sysctl -p 
</code></pre>

<p>客户端程序是基于libevent写的一个测试程序，不断的建立新的连接请求。</p>

<h2>3. 由于客户端与服务端需要建立大量的socket，所以我们需要调速一下最大文件描述符。</h2>

<p>客户端，需要创建六万多个socket，我设置最大为十万好了，的在/etc/security/limits.conf中添加：</p>

<pre><code>admin    soft    nofile  100000  
admin    hard    nofile  100000  
</code></pre>

<p>服务端，需要创建200万连接，那我想设置nofile为200万，好，问题来了。<br/>
当我设置nofile为200万时，系统直接无法登陆了。尝试几次，发现最大只能设置到100万。在查过源码后，才知道，原来在2.6.25内核之前有个宏定义，定义了这个值的最大值，为1024*1024，正好是100万，而在2.6.25内核及其之后，这个值是可以通过/proc/sys/fs/nr_open来设置。于是我升级内核到2.6.32。ulimit详细介绍见博文：<a href="http://blog.yufeng.info/archives/1380">老生常谈: ulimit问题及其影响</a>。<br/>
升级内核后，继续我们的调优，如下：</p>

<pre><code>sudo bash -c 'echo 2000000 &gt; /proc/sys/fs/nr_open' 
</code></pre>

<p>现在再设置nofile就可以了:</p>

<pre><code>admin    soft    nofile  2000000  
admin    hard    nofile  2000000 
</code></pre>

<h2>4. 最后，在测试的过程中，根据dmesg的系统打出的信息不断调整服务端/sbin/sysctl中的配置，最后我们的测试完成了200万的长连接。</h2>

<p>为了使内存占用尽量减少，我将Nginx的request_pool_size从默认的4k改成1k了。另外，net.ipv4.tcp_wmem与net.ipv4.tcp_rmem中的默认值也设置成4k。</p>

<p>两百万连接时，通过nginx的监控得到数据：<br/>
<img src="/wp-content/uploads/2011/07/abc.jpg" alt="data" /><br/>
两百万连接时系统内存情况：<br/>
<img src="/wp-content/uploads/2011/07/2.png" alt="mem" /></p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Categories</h1>
    <ul id="category-list"><li class='category'><a href='/blog/categories/http/'>http (1)</a></li>
<li class='category'><a href='/blog/categories/linux/'>linux (2)</a></li>
<li class='category'><a href='/blog/categories/nginx/'>nginx (2)</a></li>
</ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2012/12/19/slab-usage-alloc-larger-memory.html">nginx中slab分配大内存的陷阱</a>
      </li>
    
      <li class="post">
        <a href="/blog/2011/12/17/nginx-varibles.html">nginx源码分析之变量</a>
      </li>
    
      <li class="post">
        <a href="/blog/2011/09/09/port-reuse.html">端口重用引发的悲剧</a>
      </li>
    
      <li class="post">
        <a href="/blog/2011/08/29/net-work.html">统计网卡流量</a>
      </li>
    
      <li class="post">
        <a href="/blog/2011/07/07/200-long-connection.html">http长连接200万尝试及调优</a>
      </li>
    
  </ul>
</section>

<section>
<p>
<a id="hover_pop" href="http://weibo.com/lifeibo"  wb_user_id="1742220335" >淘李子@Weibo<span id="wb_follow_btn"> </a>
    </p>
    <script src="http://tjs.sjs.sinajs.cn/open/api/js/wb.js?appkey=1166443256" type="text/javascript"> </script>
    <script type="text/javascript">
        WB2.anyWhere(function(W){
                W.widget.hoverCard({
id: "hover_pop"
});


                W.widget.followButton({
uid: '1742220335',
id: "淘李子"
});
                });

</script>
</section>
<section>
  <h1>Friends links</h1>
  <ul>
      <li><a href="http://blog.yufeng.info/?from=lizi" title="erlang非业余研究" target="_blank">erlang非业余研究</a></li>
      <li><a href="http://zhuzhaoyuan.com/?from=lizi" title="叔度个人博客">叔度个人博客</a></li>
      <li><a href="http://www.imagerabit.com/?from=lizi" title="图像兔" target="_blank">图像兔</a></li>
      <li><a href="http://rdc.taobao.com/blog/cs/?from=lizi" title="淘宝核心系统博客" target="_blank">淘宝核心系统博客</a></li>
      <li><a href="http://www.dutor.net/?from=lizi" title="甘雨妹纸的博客">甘雨妹纸</a></li>
      <li><a href="http://www.pagefault.info/?from=lizi" title="雕梁nginx和内核" target="_blank">雕梁nginx和内核</a></li>
      <li><a href="http://yzprofile.me/?from=lizi" title="袁茁的小空间" target="_blank">袁茁的空间</a></li>
  </ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - lifeibo -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

<script src="/javascripts/highlight.js"></script>
<script type="text/javascript">
    hljs.tabReplace = '<span class="indent">\t</span>';
    hljs.initHighlightingOnLoad();
</script>
</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'lifeibo';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
